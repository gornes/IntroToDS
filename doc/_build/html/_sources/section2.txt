*****************
Linear Regression
*****************

The second part of this work deals with the use of tools related to machine
learning: can we use the data to create models that will allows us to predict
the ridership?

Problem Set 3 of the class has as one of the main goals the use of a linear
regression model that could help us to predict the ridership in the NYC subway.
We were asked to implement one of the algorithms that calculates the coefficients
of a multiple linear regression model: gradient descent. The selection of the
features, and thus the number of coefficients to fit, was left as an exercise
for the student.

After implementing the linear regression model, and study its strengths and
shortcomings, we used another algorithm to find the coefficients of the linear
regression model: OLS, or ordinary least squares.

Finally, a third method was used, also based on a linear regression algorithm,
but this time the model used higher order polynomials to learn from the data,
in an effort to fit the non-linearity of it.

Linear regression algorithm(s)
==============================

Gradient descent
----------------

The code used to implement the gradient descent algorithm to find the linear
regression coefficient can be checked on the python file available at the
github repository associated to this work, and on the submissions to the
*Introduction to Data Science* class (problem set 3).

Ordinary Least Squares (with statsmodels)
-----------------------------------------

Selecting the same features as in the Gradient Descent exercise, we calculated
the coefficients of the linear model by using the OLS implementation of the
statsmodels python library.


Polynomial features with Ridge linear regression
------------------------------------------------

After analysing the results from the previous regressions, and for reasons that
will be clear after the description of their results, we went a little further
and we used a polynomial transformation of the selected features, and another
linear regression algorithm, the Ridge regression, was used to model the data
and predict ridership. The model used and results will be shown in the
interpretation section.


Models and features used
========================

The selection of the features to use and how to use them in the model was not
a linear process, but an iterative work based on exploratory statistics, residuals
analysis and study of the models obtained to predict the ridership. If fact,
for the problem set 3 in the class we used a different set of variables as
predictors (specifically `day_week` instead of `weekday`).

Also, because of these iterative analysis we decided to also give a try to a
third method to model the data, which was the use of polynomial features.

The multiple regression model used for the first two methods can be written as:

.. _multreg_mod:
.. math:: \hat y = \theta_0 + \theta_1 x_1 + theta_2 x_2 + ... + \theta_k x_k
   :label: multreg_mod

where :math:`\hat y` is the predicted variable, :math:`x_i` are the predictors
(features) and :math:`\theta_i` are the coefficients or parameters that we are
looking for using either the gradient descent or the OLS algorithms.

For our work, we decided to use the following predictors:

* `UNIT`: turnstile unique identification. The use of the identification
  of each turnstile starts from the realization that the turnstiles have different
  ridership volumes for the same time periods, as it can be readily seen by
  comparing ridership averages. However this is a non-numerical categorical
  variable, so it was required to transform this variable to numerical format
  by using dummy variables. This steps adds at once `n` extra features or predictors,
  one for each turnstile in our data, which adds a lot of computing work to the
  algorithm.

* `hour`: numerical variable that indicates the hour of the day when the ridership
  was reported for each turnstile. This variable takes values that can be continuous
  between 0 and 24; it adds one coefficient to calculate. :ref:`Figure 3.1 <figure31>`
  shows the relation of the ridership values with hour the day.

.. _figure31:
.. figure:: hour_rider.png
   :scale: 80%
   :align: center

   Ridership vs hour of the day.

   Instead of just constructing a scatter plot, we decided to use another descriptive
   statistic method to study the relation, if any, between ridership and hour of
   the day. We used boxplots to visualize the distribution of ridership for each
   hour of the day. In this way we can see that the medians do not follow a
   linear relation with the hour of the day, and that there is a huge spread
   of possible ridership values for each hour.

* `weekday`: numerical (and categorical) variable indicating if the day
  when the ridership measurement was done was either a weekday (1) or weekend
  day (0). :ref:`Figure 3.2 <figure32>`
  shows the relation of the ridership with kind of day.

.. _figure32:
.. figure:: weekday_rider.png
   :scale: 80%
   :align: center

   Ridership vs weekday/weekend-day.

   This plot show the ridership distribution as boxplots for work days (Monday to
   Friday) and weekend days (Saturdays and Sundays). It is clear that even
   when the spread in entries per hour is still big a linear correlation can
   be used.

* `rain`: daily precipitation condition for a turnstile location (0 for a clear
  day, 1 for rainy). Even when is a categorical variable, it is also numerical,
  and it is used as the final predictor feature for our liner model.
  :ref:`Figure 3.3 <figure33>` shows the relation of the ridership values with
  precipitation conditions.

.. _figure33:
.. figure:: rain_rider.png
   :scale: 80%
   :align: center

   Ridership vs rainy conditions.

   With the use of boxplots again, we can see in this figure that a really mild
   linear relation exist for the relation between daily precipitation conditions
   and ridership. (Which as was shown in the previous section is not significant)

The features were selected based partially on intuition and partially by exploratory
analysis.

First, it was clear that the behavior, for each individual turnstile, was mainly
a function of the hour of the day and the day of the week, as is shown in
:ref:`Figure 3.4 <figure34>`: there is a clear periodicity in the ridership
behavior for each day, depending on the time of the day, and also a dependence
on the day of the week. However the relation is clearly non-linear. We kept the
`hour` as a predictor because is an important predictor, an in a very rough way
one can see that ridership is lower in the beginning of the day while reaching
a pick on the evenings.

.. _figure34:
.. figure:: r084_may.png
   :scale: 100%
   :align: center

   Ridership vs date for turnstile R084.

   The figure clearly shows a periodic behavior for the ridership behavior for
   a particular turnstile, which is a function mainly of the hour of the day and
   day of the week. Ridership picks are usually seen at 20 hours, while weekends
   and holidays (May 30th) being less busy than weekdays.

However, we decided to use `weekday` instead of `day_week` (the second being the
day of the week, i.e, a number between 0 and 6, where 0 i Monday and 6 Sunday),
because the major change on ridership behavior is seen between work days and
off days (weekends), and `weekday` can be better modeled by a linear model than
`day_week` (as it can be checked on :ref:`Figure 3.5 <figure35>`)

.. _figure35:
.. figure:: day_rider.png
   :scale: 80%
   :align: center

   Ridership vs day of the week.

   This plot show the ridership distribution as boxplots for the 7 days of the
   week (0 is Monday, 6 is Sunday). We can see that even when a relation
   exist between day of the week and ridership, this relation doesn't look
   linear, and thus we decided to use `weekday` instead.


Results: coefficients and R Squared
===================================

The coefficients found with the gradient descent and OLS algorithms were the
same in both cases, which was expected for a successful execution of the
gradient descent algorithm. The selected features were enough to obtain a
:math:`R^2 = 0.481`. More in depth details of the result can be seen in
:ref:`Table 3.1 <table31>`. Also, thanks to the statsmodels OLS implementation
we can report some of the coefficients obtained from the linear model fit,
using the predictor variables `hour`, `weekday`, `rain` and dummies from `UNIT`
(:ref:`Eq. 3.1 <multreg_mod>`), and their statistical significances
(:ref:`Table 3.2 <table32>`).

At :ref:`figure 3.6 <figure36>` we present some residuals analysis plots, in order
to provide later a more accurate interpretation of these results.

.. _table31:
.. table:: OLS Regression Results

   =====================================  =========================================
   OLS Regression Results
   =====================================  =========================================
   Dep. Variable:        ENTRIESn_hourly   R-squared:                       0.481
   Model:                            OLS   Adj. R-squared:                  0.478
   Method:                 Least Squares   F-statistic:                     163.1
   Date:                Wed, 07 Jan 2015   Prob (F-statistic):               0.00
   Time:                        14:12:52   Log-Likelihood:            -3.8397e+05
   No. Observations:               42267   AIC:                         7.684e+05
   Df Residuals:                   42027   BIC:                         7.705e+05
   Df Model:                         239
   Covariance Type:            nonrobust
   =====================================  =========================================

.. _table32:
.. csv-table:: Linear regression coefficients
   :header: Predictor,coef,std err,t,P>|t|,[95% Conf. Int.]
   :widths: 15, 10, 10, 10, 10, 20
   :stub-columns: 1

   **Intercept**  ,-1750.5171,  166.661,  -10.503, 0.000,-2077.175 -1423.859
   C(UNIT)[T.R004],  334.1581,  231.108,    1.446, 0.148, -118.819   787.135
   C(UNIT)[T.R005],  335.0522,  232.095,    1.444, 0.149, -119.859   789.963
   C(UNIT)[T.R006],  451.3319,  229.532,    1.966, 0.049,    1.445   901.218
   C(UNIT)[T.R007],  164.5844,  232.767,    0.707, 0.480, -291.644   620.812
   ...            ,       ...,      ...,      ...,   ...,      ...       ...
   **hour**       ,  124.0989,    1.500,   82.741, 0.000,  121.159   127.039
   **weekday**    ,  980.9091,   23.243,   42.203, 0.000,  935.353  1026.465
   **rain**       ,   36.3145,   25.167,    1.443, 0.149,  -13.013    85.642

.. _figure36:
.. figure:: residuals_an.png
   :scale: 100%
   :align: center

   Residuals analysis plots for the linear regression model (improved dataset).

   *Top left:* normal probability plot of the residuals and *top right:* residuals
   distribution. It is clear that residuals do not adjust well to a simple normal
   probability distribution. *Bottom left* shows the residuals versus the
   predicted quantities, and *bottom right* just shows the residuals distribution
   around 0.


Interpretation and limits
=========================

.. _figure37:
.. figure:: turns_pred.png
   :scale: 90%
   :align: center

   The observed ridership values for two different turnstiles in the month of May,
   over-plotted with the predicted ridership for both cases.

   The turnstiles used were R084 and R172, one at downtown and the other at the
   periphery. The predicted values come from the linear regression model applied
   in previous section.

A successful multiple regression model should comply with several assumptions,
which can be checked by analysing the residuals.

1.